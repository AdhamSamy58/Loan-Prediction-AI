
#! Project Ai Loan prediction

#TODO libraries # pyline: disable=fixmeP
import pandas as pd # dataframes
import seaborn as sns # to visualize & numerate data
import matplotlib.pyplot as plt # to create graphs
import numpy as np # deals with numbers
from sklearn.model_selection import train_test_split #splits data to x_train ,y_train,x_test,y_test
from sklearn.preprocessing import StandardScaler # scales data: upperbound =1,  lower bound =0
from sklearn import metrics #for printing accuracy 
from sklearn.metrics import accuracy_score
from sklearn.feature_selection import GenericUnivariateSelect, chi2, f_classif
#################### models import ##########################
from sklearn import svm
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.naive_bayes import BernoulliNB
from sklearn.naive_bayes import GaussianNB
from sklearn.neighbors import KNeighborsClassifier



#TODO getting data :-

train_df = pd.read_csv('loan_data.csv')
train_df = train_df.drop(columns=['Loan_ID']) 
categorical_columns = ['Gender', 'Married', 'Dependents', 'Education',
 'Self_Employed', 'Property_Area','Credit_History','Loan_Amount_Term']
numerical_columns = ['ApplicantIncome', 'CoapplicantIncome', 'LoanAmount']


#TODO plotting data in graphs :-

#*categorical data : 
fig,axes = plt.subplots(4,2,figsize=(12,15)) #making empty graphs
for idx,cat_col in enumerate(categorical_columns): #converting data to numbers // #cat_col:column(attribute) to be itterated
    row,col = idx//2,idx%2
    sns.countplot(x=cat_col,data=train_df,hue='Loan_Status',ax=axes[row,col]) #giving parameters to Bar Chart graph
plt.subplots_adjust(hspace=0.5) #adjusting space between graphs
plt.show()

#*numerical data :
fig,axes = plt.subplots(1,3,figsize=(25,5)) #making empty graphs
for idx,cat_col in enumerate(numerical_columns): #converting data to numbers
    sns.boxplot(y=cat_col,data=train_df,x='Loan_Status',ax=axes[idx]) #giving parameters to Box Plot graph
plt.subplots_adjust(wspace=0.3) #adjusting space between graphs
plt.show()


#TODO data cleaning:-

#filling null values of the train_df:
train_df["Gender"].fillna(train_df["Gender"].mode()[0], inplace=True)
train_df["Married"].fillna(train_df["Married"].mode()[0],inplace=True)
train_df["Dependents"].fillna(train_df["Dependents"].mode()[0],inplace=True)
train_df["Self_Employed"].fillna(train_df["Self_Employed"].mode()[0],inplace=True)
train_df["Loan_Amount_Term"].fillna(train_df["Loan_Amount_Term"].mode()[0],inplace=True)
train_df["Credit_History"].fillna(train_df["Credit_History"].mode()[0],inplace=True)

train_df["LoanAmount"].fillna(train_df["LoanAmount"].mean(),inplace=True)


#TODO encoding categorical data to numbers:-

train_df_encoded = pd.get_dummies(train_df,drop_first=True) #drop_first:drop 1 attribute to make k-1 attribute ,drop_first=True
# *maslan attribute gender msh lazm n3ml male w female n3ml male bas w hwa law male fa akeed female  
pd.set_option('display.max_columns', train_df_encoded.shape[0]+1)
# * columns:-[ApplicantIncome,CoapplicantIncome,LoanAmount,Loan_Amount_Term,
# *           Credit_History,Gender_Male,Married_Yes,Dependents_1,Dependents_2,
# *           Dependents_3,Education_Not Graduate,Self_Employed_Yes,
# *           Property_Area_Semiurban,Property_Area_Urban,Loan_Status_Y]


#TODO dividing data to independant(x) and dependant (y):-
x = train_df_encoded.drop(columns='Loan_Status_Y') # all columns but (Loan_Status_Y)
y = train_df_encoded['Loan_Status_Y']


# TODO feature selection:-
#feature selection is added here
x = GenericUnivariateSelect(score_func=chi2, mode="k_best", param=8).fit_transform(x, y)


#TODO dividing data to train and test samples :-
X_train,X_test,y_train,y_test = train_test_split(x,y,test_size=0.3,stratify = y,random_state =250)



#TODO scalling:-
sc_X = StandardScaler() 
X_train = sc_X.fit_transform(X_train)
X_test = sc_X.fit_transform(X_test)



#!############################################## MODELS ################################################



#TODO SVM(Support Vector Machine) model:-

#*linear :
print("SVM:-")

clf=svm.SVC(kernel='linear')
clf.fit(X_train,y_train)
y_pred=clf.predict(X_test)
print("Linear SVM Accuracy=",metrics.accuracy_score(y_test,y_pred))

# #*poly :
# svm_degree2=svm.SVC(kernel='poly',degree=2)
# svm_degree2.fit(X_train,y_train)
# y_pred=svm_degree2.predict(X_test)
# print("poly SVM Accuracy=",metrics.accuracy_score(y_test,y_pred),'\n')




#TODO logistic regression:-
print('\nlogistic regression:-')

model = LogisticRegression(solver='liblinear',C=10.0,random_state=0)
model.fit(X_train,y_train)
y_pred = model.predict(X_test)
print('score:', accuracy_score(y_test,y_pred),'\n')



#TODO Dession tree:-
print('decsion tree:-')
#*Entropy:
clf = DecisionTreeClassifier(criterion="entropy")
clf.fit(X_train, y_train)
y_pred = clf.predict(X_test)
print("Decision tree accuracy using entropy:", accuracy_score(y_test,y_pred))

#*gini:
clf = DecisionTreeClassifier(criterion="gini")
clf.fit(X_train, y_train)
y_pred = clf.predict(X_test)
print("Decision tree accuracy using gini:", accuracy_score(y_test,y_pred))



#TODO NB:-
print('\nnaive bayes:-')
#*bernoulli:
classifer = BernoulliNB()
classifer.fit(X_train, y_train)
y_pred = classifer.predict(X_test)
print('Bernoulli NB:' , accuracy_score(y_test,y_pred))

# #* gaussian:
# classifer1 = GaussianNB()
# classifer1.fit(X_train, y_train)
# y_pred1 = classifer1.predict(X_test)
# print('gaussiaan NB:' , accuracy_score(y_pred1,y_test))



#TODO knn:-  
classifier2= KNeighborsClassifier(n_neighbors=5, metric='minkowski', p=2 )  
classifier2.fit(X_train, y_train)  
y_pred= classifier2.predict(X_test)  
print( '\nknn :' , accuracy_score(y_test,y_pred))

